# Discussion

## 5.1 Interpretation of RL Policy Behavior

The RL-derived policy shows weak sensitivity to state variables. This is not a failure of the algorithm but a reflection of:

- Strong intrinsic population growth  
- Limited leverage of release and mitigation actions  
- High environmental stochasticity  
- Nonlinear, piecewise-constant Q approximators  

When ecological dynamics dominate management actions, optimal policies often stabilize into cost-minimizing behavior with minimal intervention.

---

## 5.2 Why Does the Population Stay Stable?

The calibrated environment includes:

- Moderate survival  
- Nontrivial reproduction  
- Habitat recovery processes  

These factors produce a net-growth regime, meaning population tends to increase unless disrupted by rare disasters. This explains the smooth upward trajectories seen in simulations.

---

## 5.3 Disaster Events as Dominant Drivers

With a 3% annual disaster probability, large population declines occur approximately every 33 years. This aligns with observed major crashes in simulations.

Importantly:

- RL cannot prevent disasters  
- RL does not disproportionately release birds in anticipation  
- Recovery occurs naturally due to model reproduction assumptions  

---

## 5.4 Limitations

Several modeling assumptions may limit realism:

1. Lead risk is simplified and may underestimate cumulative effects.  
2. Habitat index H is aggregated and does not represent spatial variation.  
3. The random forest Q approximator produces discontinuous policies.  
4. Reward function emphasizes population targets but not genetic diversity or spatial distribution.  
5. Behavioral stochasticity of condors is not modeled explicitly.

---

## 5.5 Future Work

Potential extensions include:

- Neural network–based Q-learning for smoother policies  
- Policy gradient or actor–critic methods  
- Multi-objective RL including cost, risk, and genetic health  
- Spatially explicit metapopulation models  
- Bayesian or empirical calibration using field data  

These directions would bring RL management strategies closer to field-ready applications.
