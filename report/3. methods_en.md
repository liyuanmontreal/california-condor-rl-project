# Methods

## 3.1 Age-Structured Population Model

We adopt a five-dimensional state representation:

s = [Nj, Ns, Na, C, H]

where:

- **Nj**: Juveniles (0–2 years)
- **Ns**: Subadults (2–6 years)
- **Na**: Adults (≥6 years, breeding)
- **C**: Captive population
- **H**: Habitat quality index (baseline = 1.0)

Management actions are:

a = [release, mitigation]


### Population dynamics include:
- Age transitions with survival probabilities  
- Adult reproduction with habitat sensitivity  
- Lead poisoning mortality conditioned on mitigation  
- Density- and habitat-dependent release survival  
- Stochastic disaster events affecting all wild birds  
- Logistic captive population growth  
- Habitat recovery and improvement through mitigation  

All parameters are calibrated using real-history data.

---

## 3.2 Reward Function

The management objective is:

- Maintain wild population within a target window:  
- 
target_low = 650, target_high = 700

- Penalize deviation quadratically  
- Penalize intervention costs:


R =

w_low * max(0, target_low - N_total)^2

w_high * max(0, N_total - target_high)^2

λ_release * release

λ_mitigation * mitigation

λ_disaster * 1(disaster_event)


---

## 3.3 Reinforcement Learning Framework

We use Fitted Q-Iteration (FQI) with Random Forest regression.

### Training procedure:
1. Collect dataset from random policy:  
   300 episodes × 100-year horizon = 30,000 transitions
2. Construct an action grid:  
0–20 release × 0.0–1.0 mitigation

3. For each iteration:
- Fit Q(s, a) on bootstrapped targets  
- Update greedy policy  
4. Final policy = greedy(Q)

Advantages:
- Handles large state spaces  
- Robust to noise  
- Works with offline data  
- Produces interpretable piecewise-constant policies  

---

## 3.4 Policy Visualization

To interpret RL behavior, we compute:

- **Population trajectories** (Nj, Ns, Na, N_total)
- **Action trajectories**
- **Phase portraits**: release(N_total), mitigation(N_total)
- **KDE density plots**
- **Hexbin density plots**
- **2D state-grid heatmaps**:  
release(N_total, C), mitigation(N_total, C)

These visualizations reveal the structure and sensitivity of the learned policies.

