# 方法

## 3.1 年龄结构种群模型

我们采用五维状态表示：

s = [Nj, Ns, Na, C, H]


其中：

- **Nj**：幼年组（0–2 岁）  
- **Ns**：亚成体（2–6 岁）  
- **Na**：成体（≥6 岁，具有繁殖能力）  
- **C**：圈养种群  
- **H**：栖息地质量指标（基线 = 1.0）

管理动作为：


a = [release, mitigation]

### 种群动力学包括：

- 生存与年龄递迁  
- 成鸟繁殖（受栖息地影响）  
- 铅中毒死亡（随治理力度变化）  
- 放飞成功率（随密度与栖息地变化）  
- 随机灾害事件（影响所有野外个体）  
- 圈养种群的 logistic 增长  
- 栖息地恢复与治理改善  

所有参数均基于历史数据进行校准。

---

## 3.2 奖励函数

目标是：

- 使野外种群保持在目标区间内：  
target_low = 650, target_high = 700
- 偏离目标将被二次惩罚  
- 干预动作（放飞、治理）都有成本

奖励定义为：

R =

w_low * max(0, target_low - N_total)^2

w_high * max(0, N_total - target_high)^2

λ_release * release

λ_mitigation * mitigation

λ_disaster * 1(disaster_event)


---

## 3.3 强化学习：Fitted Q-Iteration（FQI）

采用随机森林回归器学习 Q(s, a)。

### 训练流程：

1. 使用随机策略采样：
   300 个 episode × 100 年 horizon = 30,000 数据点  
2. 构建动作网格：
release = 0–20, mitigation = 0.0–1.0

3. 每次迭代：
- 基于 Bellman 目标拟合 Q  
- 更新贪心策略  
4. 最终策略为 Q 的贪心动作

优势：

- 适用于高维状态  
- 抗噪声能力强  
- 可离线训练  
- 产生“分段常数”策略，便于解释  

---

## 3.4 策略可视化

为解释 RL 行为，我们绘制：

- **种群轨迹**（Nj, Ns, Na, N_total）  
- **动作轨迹**  
- **相图** release(N_total), mitigation(N_total)  
- **KDE 密度图**  
- **Hexbin 密度图**  
- **状态网格 2D 热力图**：  
release(N_total, C), mitigation(N_total, C)

这些图揭示了策略的结构性与敏感性。
