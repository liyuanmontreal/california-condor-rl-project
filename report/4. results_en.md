# Results

## 4.1 Phase 2: Policy Simulation

We compare four management scenarios:

1. **Zero policy (no release, no mitigation)**  
2. **Literature policy** based on parameters from published ecological studies  
3. **Optimistic policy** with higher survival and reproduction  
4. **Calibrated policy** based on real-history data  

Results show:

- The *zero* policy leads to long-term decline, validating model realism.
- The *literature* and *optimistic* policies both produce stable growth.
- The *calibrated* simulation reproduces the historical decline observed in real condor recovery data (1980–2010), demonstrating successful parameter calibration.

This establishes the calibrated model as a realistic environment for RL training.

---

## 4.2 Phase 3: RL Population Trajectories

Using the trained FQI policy:

- Juvenile (Nj), subadult (Ns), and adult (Na) groups all show gradual, stable increase.
- Total wild population rises smoothly except for rare, sharp declines.
- These declines correspond to stochastic disasters, which occur with probability ~3% per year.
- The model predicts one major crash every 30–40 years, matching the disaster module’s design.

Despite stochasticity, trajectories remain stable due to strong reproduction and habitat recovery dynamics.

---

## 4.3 Action Trajectories

The RL policy exhibits:

- **Release actions** that fluctuate among discrete values (0, 5, 10, 12, 15, 20).
- **Mitigation actions** that cluster at preferred levels (0.0, 0.4, 0.6, 1.0).

These behaviors arise naturally from the piecewise-constant approximation of Random Forest Q-learning.

Notably:

- There is **no simple monotonic pattern** such as “release more when population is low.”
- Intervention costs strongly influence learned actions.
- The system’s inherent growth dynamics dominate, resulting in weak state dependence.

---

## 4.4 Phase Portraits (Release & Mitigation vs N_total)

Scatterplots, KDE, hexbin, and heatmaps consistently reveal:

- Release decisions are widely scattered across N_total values.
- Mitigation actions form horizontal bands at specific preferred values.
- Neither action shows strong sensitivity to population size.

This indicates a multi-modal policy rather than a smooth control function.

---

## 4.5 2D State-Grid Heatmaps

By evaluating the policy over a grid of states (N_total × C):

- Release heatmaps show vertical band patterns, demonstrating **C has minimal effect** on decisions.
- Mitigation heatmaps show only a few distinct stripes, confirming discretized preferred mitigation levels.
- Both heatmaps reinforce the weak state dependence and cost-driven nature of the learned policy.

These heatmaps match qualitative patterns seen in ecological RL literature.

---

## 4.6 Summary of Results

1. **Calibrated model is realistic**, reproducing historical population trends.  
2. **RL policies are stable**, providing long-term growth under stochastic disturbance.  
3. **Learned strategies are cost-sensitive**, relying less on aggressive release.  
4. **Policy structure is multi-modal**, arising from Random Forest approximation.  
5. **Environmental stochasticity dominates**, making population trajectories smooth and predictable except for rare disasters.

Overall, the RL policy is biologically reasonable and robust to uncertainty.
